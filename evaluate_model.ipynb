{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ceac4fb",
   "metadata": {},
   "source": [
    "# Model Evaluation - Informal to Formal Text Transformation\n",
    "\n",
    "This notebook evaluates the trained model using various metrics:\n",
    "- **BLEU Score**: Measures n-gram overlap with reference translations\n",
    "- **ROUGE Scores**: Measures recall-oriented overlap (ROUGE-1, ROUGE-2, ROUGE-L)\n",
    "- **Perplexity**: Measures model confidence\n",
    "- **Custom Metrics**: Formality score, length preservation, semantic similarity\n",
    "\n",
    "## Requirements\n",
    "Make sure you have trained a model using `train_model.ipynb` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ac7868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "!pip install transformers datasets evaluate torch sacrebleu rouge-score nltk pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dd4620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from datasets import load_dataset, Dataset\n",
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ffe844",
   "metadata": {},
   "source": [
    "## 1. Load Model and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30114a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_PATH = \"./models/informal-to-formal-t5\"  # Path to your trained model\n",
    "TEST_DATA_PATH = \"./data/informal_formal/test.json\"  # Path to test data\n",
    "\n",
    "# Check if model exists\n",
    "if not Path(MODEL_PATH).exists():\n",
    "    print(f\"WARNING: Model not found at {MODEL_PATH}\")\n",
    "    print(\"Using pretrained model instead: google/flan-t5-small\")\n",
    "    MODEL_PATH = \"google/flan-t5-small\"\n",
    "else:\n",
    "    print(f\"âœ“ Model found at {MODEL_PATH}\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "print(\"Loading model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model loaded on {device}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c29a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "if Path(TEST_DATA_PATH).exists():\n",
    "    with open(TEST_DATA_PATH, 'r') as f:\n",
    "        test_data = json.load(f)\n",
    "    print(f\"âœ“ Loaded {len(test_data)} test samples\")\n",
    "else:\n",
    "    print(f\"WARNING: Test data not found at {TEST_DATA_PATH}\")\n",
    "    print(\"Creating sample test data...\")\n",
    "    test_data = [\n",
    "        {\"informal\": \"send me the file now\", \"formal\": \"Could you please send me the file at your earliest convenience?\"},\n",
    "        {\"informal\": \"why didn't you finish the report?\", \"formal\": \"I noticed the report is incomplete. Could you please provide an update on its status?\"},\n",
    "        {\"informal\": \"need this asap\", \"formal\": \"I would appreciate receiving this as soon as possible.\"},\n",
    "        {\"informal\": \"what's taking so long?\", \"formal\": \"May I inquire about the current progress?\"},\n",
    "        {\"informal\": \"good job\", \"formal\": \"Excellent work on this project.\"},\n",
    "    ]\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample test data:\")\n",
    "for i, item in enumerate(test_data[:3]):\n",
    "    print(f\"{i+1}. Informal: {item['informal']}\")\n",
    "    print(f\"   Formal: {item['formal']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a52188",
   "metadata": {},
   "source": [
    "## 2. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614ecb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction(informal_text, max_length=128, num_beams=4):\n",
    "    \"\"\"Generate formal text from informal text.\"\"\"\n",
    "    prompt = f\"Convert informal to formal: {informal_text}\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            do_sample=False,\n",
    "        )\n",
    "    \n",
    "    prediction = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    return prediction\n",
    "\n",
    "# Generate predictions for all test samples\n",
    "print(\"Generating predictions...\")\n",
    "predictions = []\n",
    "references = []\n",
    "inputs = []\n",
    "\n",
    "for item in tqdm(test_data):\n",
    "    informal_text = item['informal']\n",
    "    formal_text = item['formal']\n",
    "    \n",
    "    prediction = generate_prediction(informal_text)\n",
    "    \n",
    "    inputs.append(informal_text)\n",
    "    predictions.append(prediction)\n",
    "    references.append(formal_text)\n",
    "\n",
    "print(f\"\\nâœ“ Generated {len(predictions)} predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c1df03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some examples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(min(5, len(test_data))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Input (Informal):  {inputs[i]}\")\n",
    "    print(f\"  Reference (Formal): {references[i]}\")\n",
    "    print(f\"  Prediction:         {predictions[i]}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adea579",
   "metadata": {},
   "source": [
    "## 3. Calculate BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6062a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BLEU metric\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "# Calculate BLEU score\n",
    "references_list = [[ref] for ref in references]  # sacrebleu expects list of lists\n",
    "bleu_result = bleu_metric.compute(predictions=predictions, references=references_list)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BLEU SCORE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"BLEU Score: {bleu_result['score']:.2f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  > 40: Excellent translation quality\")\n",
    "print(f\"  30-40: Good translation quality\")\n",
    "print(f\"  20-30: Acceptable translation quality\")\n",
    "print(f\"  < 20: Poor translation quality\")\n",
    "print(f\"\\nYour model: {bleu_result['score']:.2f} - \", end=\"\")\n",
    "if bleu_result['score'] > 40:\n",
    "    print(\"Excellent! ðŸŽ‰\")\n",
    "elif bleu_result['score'] > 30:\n",
    "    print(\"Good! ðŸ‘\")\n",
    "elif bleu_result['score'] > 20:\n",
    "    print(\"Acceptable âœ“\")\n",
    "else:\n",
    "    print(\"Needs improvement ðŸ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739ef324",
   "metadata": {},
   "source": [
    "## 4. Calculate ROUGE Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4362f341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ROUGE metric\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "rouge_result = rouge_metric.compute(\n",
    "    predictions=predictions,\n",
    "    references=references,\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ROUGE SCORES\")\n",
    "print(\"=\"*80)\n",
    "print(f\"ROUGE-1 (unigram overlap):  {rouge_result['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2 (bigram overlap):   {rouge_result['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L (longest common):   {rouge_result['rougeL']:.4f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  ROUGE-1: Measures word-level overlap (higher is better)\")\n",
    "print(f\"  ROUGE-2: Measures phrase-level overlap (higher is better)\")\n",
    "print(f\"  ROUGE-L: Measures fluency and coherence (higher is better)\")\n",
    "print(f\"  Good scores are typically > 0.5 for ROUGE-1 and > 0.3 for ROUGE-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d9082e",
   "metadata": {},
   "source": [
    "## 5. Calculate Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b098475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(texts):\n",
    "    \"\"\"Calculate average perplexity for a list of texts.\"\"\"\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Calculating perplexity\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(device)\n",
    "            \n",
    "            # For decoder models, labels should be the same as input_ids\n",
    "            labels = inputs.input_ids.clone()\n",
    "            \n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            total_loss += loss.item() * inputs.input_ids.size(1)\n",
    "            total_tokens += inputs.input_ids.size(1)\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "# Calculate perplexity on predictions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERPLEXITY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    perplexity = calculate_perplexity(predictions[:50])  # Calculate on first 50 to save time\n",
    "    print(f\"Perplexity: {perplexity:.2f}\")\n",
    "    print(f\"\\nInterpretation:\")\n",
    "    print(f\"  Lower perplexity = better model confidence\")\n",
    "    print(f\"  < 50: Excellent\")\n",
    "    print(f\"  50-100: Good\")\n",
    "    print(f\"  100-200: Acceptable\")\n",
    "    print(f\"  > 200: Poor\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not calculate perplexity: {e}\")\n",
    "    perplexity = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b5ade7",
   "metadata": {},
   "source": [
    "## 6. Custom Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length preservation (how well does the model preserve input length?)\n",
    "input_lengths = [len(text.split()) for text in inputs]\n",
    "prediction_lengths = [len(text.split()) for text in predictions]\n",
    "reference_lengths = [len(text.split()) for text in references]\n",
    "\n",
    "# Calculate average length ratios\n",
    "pred_to_input_ratio = np.mean([p/i if i > 0 else 0 for p, i in zip(prediction_lengths, input_lengths)])\n",
    "pred_to_ref_ratio = np.mean([p/r if r > 0 else 0 for p, r in zip(prediction_lengths, reference_lengths)])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LENGTH ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Average input length:      {np.mean(input_lengths):.1f} words\")\n",
    "print(f\"Average prediction length: {np.mean(prediction_lengths):.1f} words\")\n",
    "print(f\"Average reference length:  {np.mean(reference_lengths):.1f} words\")\n",
    "print(f\"\\nLength ratios:\")\n",
    "print(f\"  Prediction/Input:    {pred_to_input_ratio:.2f}x\")\n",
    "print(f\"  Prediction/Reference: {pred_to_ref_ratio:.2f}x\")\n",
    "print(f\"\\nNote: Formal text is typically 2-3x longer than informal text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9a55b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word-level analysis\n",
    "# Check for informal words in predictions\n",
    "informal_words = {\n",
    "    'u', 'ur', 'pls', 'thx', 'gonna', 'wanna', 'gotta', 'kinda', 'sorta',\n",
    "    'dunno', 'gimme', 'lemme', 'yeah', 'yep', 'yup', 'nope', 'nah',\n",
    "    'asap', 'fyi', 'btw', 'omg', 'lol', 'brb', 'gtg'\n",
    "}\n",
    "\n",
    "def count_informal_words(text):\n",
    "    words = text.lower().split()\n",
    "    return sum(1 for word in words if word in informal_words)\n",
    "\n",
    "informal_in_inputs = [count_informal_words(text) for text in inputs]\n",
    "informal_in_predictions = [count_informal_words(text) for text in predictions]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FORMALITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Informal words in inputs:      {sum(informal_in_inputs)}\")\n",
    "print(f\"Informal words in predictions: {sum(informal_in_predictions)}\")\n",
    "print(f\"Formalization rate: {(1 - sum(informal_in_predictions) / max(sum(informal_in_inputs), 1)) * 100:.1f}%\")\n",
    "print(f\"\\nNote: Good models should remove most informal words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab658dd1",
   "metadata": {},
   "source": [
    "## 7. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ee457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'Input': inputs,\n",
    "    'Reference': references,\n",
    "    'Prediction': predictions,\n",
    "    'Input_Length': input_lengths,\n",
    "    'Prediction_Length': prediction_lengths,\n",
    "    'Reference_Length': reference_lengths,\n",
    "})\n",
    "\n",
    "print(\"\\nResults DataFrame:\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564dab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Length comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Length distribution\n",
    "axes[0].hist([input_lengths, prediction_lengths, reference_lengths], \n",
    "             label=['Input', 'Prediction', 'Reference'],\n",
    "             alpha=0.6, bins=15)\n",
    "axes[0].set_xlabel('Text Length (words)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Text Length Distribution')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Length correlation\n",
    "axes[1].scatter(reference_lengths, prediction_lengths, alpha=0.6)\n",
    "axes[1].plot([0, max(reference_lengths)], [0, max(reference_lengths)], 'r--', label='Perfect match')\n",
    "axes[1].set_xlabel('Reference Length (words)')\n",
    "axes[1].set_ylabel('Prediction Length (words)')\n",
    "axes[1].set_title('Length Correlation: Prediction vs Reference')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('length_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Length analysis plot saved as 'length_analysis.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312a8766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Metrics summary\n",
    "metrics_data = {\n",
    "    'BLEU': bleu_result['score'] / 100,  # Normalize to 0-1\n",
    "    'ROUGE-1': rouge_result['rouge1'],\n",
    "    'ROUGE-2': rouge_result['rouge2'],\n",
    "    'ROUGE-L': rouge_result['rougeL'],\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.bar(metrics_data.keys(), metrics_data.values(), color=['#3498db', '#2ecc71', '#e74c3c', '#f39c12'])\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Evaluation Metrics', fontsize=16, fontweight='bold')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.3f}',\n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('metrics_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Metrics summary plot saved as 'metrics_summary.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da7f621",
   "metadata": {},
   "source": [
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfae915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "results_df.to_csv('evaluation_results.csv', index=False)\n",
    "print(\"âœ“ Detailed results saved to 'evaluation_results.csv'\")\n",
    "\n",
    "# Save metrics summary\n",
    "metrics_summary = {\n",
    "    'model_path': MODEL_PATH,\n",
    "    'test_samples': len(test_data),\n",
    "    'bleu_score': float(bleu_result['score']),\n",
    "    'rouge1': float(rouge_result['rouge1']),\n",
    "    'rouge2': float(rouge_result['rouge2']),\n",
    "    'rougeL': float(rouge_result['rougeL']),\n",
    "    'perplexity': float(perplexity) if perplexity else None,\n",
    "    'avg_input_length': float(np.mean(input_lengths)),\n",
    "    'avg_prediction_length': float(np.mean(prediction_lengths)),\n",
    "    'avg_reference_length': float(np.mean(reference_lengths)),\n",
    "    'pred_to_input_ratio': float(pred_to_input_ratio),\n",
    "    'pred_to_ref_ratio': float(pred_to_ref_ratio),\n",
    "    'informal_words_removed': sum(informal_in_inputs) - sum(informal_in_predictions),\n",
    "    'formalization_rate': float((1 - sum(informal_in_predictions) / max(sum(informal_in_inputs), 1)) * 100),\n",
    "}\n",
    "\n",
    "with open('metrics_summary.json', 'w') as f:\n",
    "    json.dump(metrics_summary, f, indent=2)\n",
    "\n",
    "print(\"âœ“ Metrics summary saved to 'metrics_summary.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0eafdb",
   "metadata": {},
   "source": [
    "## 9. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74593846",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL EVALUATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nModel: {MODEL_PATH}\")\n",
    "print(f\"Test Samples: {len(test_data)}\")\n",
    "print(f\"\\nAutomatic Metrics:\")\n",
    "print(f\"  BLEU:    {bleu_result['score']:.2f}\")\n",
    "print(f\"  ROUGE-1: {rouge_result['rouge1']:.4f}\")\n",
    "print(f\"  ROUGE-2: {rouge_result['rouge2']:.4f}\")\n",
    "print(f\"  ROUGE-L: {rouge_result['rougeL']:.4f}\")\n",
    "if perplexity:\n",
    "    print(f\"  Perplexity: {perplexity:.2f}\")\n",
    "print(f\"\\nCustom Metrics:\")\n",
    "print(f\"  Formalization Rate: {metrics_summary['formalization_rate']:.1f}%\")\n",
    "print(f\"  Length Expansion: {pred_to_input_ratio:.2f}x\")\n",
    "print(f\"\\nFiles Generated:\")\n",
    "print(f\"  - evaluation_results.csv\")\n",
    "print(f\"  - metrics_summary.json\")\n",
    "print(f\"  - length_analysis.png\")\n",
    "print(f\"  - metrics_summary.png\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
