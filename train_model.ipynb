{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b35014",
   "metadata": {},
   "source": [
    "# Training Informal-to-Formal Text Transformer\n",
    "\n",
    "This notebook fine-tunes a T5/FLAN-T5 model on the task of converting informal text to formal text.\n",
    "\n",
    "## Dataset\n",
    "We'll use the GYAFC (Grammarly's Yahoo Answers Formality Corpus) dataset or create a synthetic dataset for informal-to-formal transformation.\n",
    "\n",
    "## Model\n",
    "- Base model: `google/flan-t5-base` (or `t5-base`)\n",
    "- Task: Sequence-to-sequence text generation\n",
    "- Training: 100 epochs with early stopping\n",
    "\n",
    "## Metrics\n",
    "- BLEU score\n",
    "- ROUGE scores (ROUGE-1, ROUGE-2, ROUGE-L)\n",
    "- Perplexity\n",
    "- Training/Validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d37daeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "!pip install transformers datasets evaluate torch accelerate tensorboard sentencepiece sacrebleu rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3861d869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "import evaluate\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c3a22f",
   "metadata": {},
   "source": [
    "## Step 1: Prepare Dataset\n",
    "\n",
    "We'll create a synthetic dataset of informal-to-formal text pairs. For a real project, you can use:\n",
    "- GYAFC dataset from: https://github.com/raosudha89/GYAFC-corpus\n",
    "- Or load from Hugging Face: `jxm/informal_to_formal`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b948b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic dataset for demonstration\n",
    "# For better results, replace with a real dataset\n",
    "\n",
    "informal_formal_pairs = [\n",
    "    # Requests\n",
    "    (\"send me the file now\", \"Could you please send me the file at your earliest convenience?\"),\n",
    "    (\"need this asap\", \"I would appreciate receiving this as soon as possible.\"),\n",
    "    (\"gimme a sec\", \"Please allow me a moment.\"),\n",
    "    (\"hey can u help?\", \"Hello, would you be able to assist me?\"),\n",
    "    (\"get back to me\", \"Please respond at your convenience.\"),\n",
    "    \n",
    "    # Questions\n",
    "    (\"why didn't you finish the report?\", \"I noticed the report is incomplete. Could you please provide an update on its status?\"),\n",
    "    (\"what's taking so long?\", \"May I inquire about the current progress?\"),\n",
    "    (\"where's the data?\", \"Could you please direct me to the location of the data?\"),\n",
    "    (\"did u see my email?\", \"Have you had the opportunity to review my email?\"),\n",
    "    (\"when can we meet?\", \"When would be a convenient time for us to schedule a meeting?\"),\n",
    "    \n",
    "    # Statements\n",
    "    (\"this is wrong\", \"I believe there may be an error in this.\"),\n",
    "    (\"that won't work\", \"I'm concerned that approach may not be effective.\"),\n",
    "    (\"u made a mistake\", \"It appears there may have been an oversight.\"),\n",
    "    (\"i'm busy right now\", \"I am currently occupied with other tasks.\"),\n",
    "    (\"can't do it today\", \"Unfortunately, I will not be able to complete this today.\"),\n",
    "    \n",
    "    # Feedback\n",
    "    (\"good job\", \"Excellent work on this project.\"),\n",
    "    (\"looks ok to me\", \"This appears to be satisfactory.\"),\n",
    "    (\"not bad\", \"This is quite good.\"),\n",
    "    (\"yeah that's fine\", \"Yes, that would be acceptable.\"),\n",
    "    (\"nah i don't think so\", \"I respectfully disagree with that assessment.\"),\n",
    "    \n",
    "    # More examples for better training\n",
    "    (\"fix it now\", \"Please address this issue at your earliest convenience.\"),\n",
    "    (\"what's up with that?\", \"Could you please clarify the situation?\"),\n",
    "    (\"i dunno\", \"I am uncertain about that.\"),\n",
    "    (\"nope\", \"No, thank you.\"),\n",
    "    (\"yup\", \"Yes, certainly.\"),\n",
    "    (\"call me\", \"Please contact me at your convenience.\"),\n",
    "    (\"let's do this\", \"Shall we proceed with this plan?\"),\n",
    "    (\"i'm not gonna do that\", \"I will not be able to proceed with that request.\"),\n",
    "    (\"whatever\", \"I understand your perspective.\"),\n",
    "    (\"who cares\", \"That may not be a primary concern.\"),\n",
    "    \n",
    "    # Extended set for better training\n",
    "    (\"got it\", \"I understand completely.\"),\n",
    "    (\"my bad\", \"I apologize for that error.\"),\n",
    "    (\"no way\", \"I find that difficult to believe.\"),\n",
    "    (\"for sure\", \"Absolutely, I agree.\"),\n",
    "    (\"hang on\", \"Please wait a moment.\"),\n",
    "    (\"lemme know\", \"Please inform me when you have an update.\"),\n",
    "    (\"gonna be late\", \"I will be arriving later than expected.\"),\n",
    "    (\"can't make it\", \"Unfortunately, I will be unable to attend.\"),\n",
    "    (\"what's the deal?\", \"What is the current situation?\"),\n",
    "    (\"cut it out\", \"Please discontinue that behavior.\"),\n",
    "    (\"knock it off\", \"Please cease that action.\"),\n",
    "    (\"chill out\", \"Please remain calm.\"),\n",
    "    (\"no biggie\", \"That is not a significant concern.\"),\n",
    "    (\"kinda busy\", \"I am somewhat occupied at the moment.\"),\n",
    "    (\"sorta like that\", \"It is somewhat similar to that.\"),\n",
    "    (\"tons of work\", \"I have a substantial amount of work.\"),\n",
    "    (\"super important\", \"This is extremely important.\"),\n",
    "    (\"really cool\", \"This is quite impressive.\"),\n",
    "    (\"that sucks\", \"That is unfortunate.\"),\n",
    "    (\"awesome job\", \"You have done exceptional work.\"),\n",
    "]\n",
    "\n",
    "# Create more variations by duplicating and slightly modifying\n",
    "extended_pairs = informal_formal_pairs.copy()\n",
    "\n",
    "# Add some variations\n",
    "variations = [\n",
    "    (\"pls send the file\", \"Please send the file at your convenience.\"),\n",
    "    (\"need help with this\", \"I would appreciate assistance with this matter.\"),\n",
    "    (\"can u check this?\", \"Could you please review this?\"),\n",
    "    (\"talk later\", \"Let us continue this conversation at a later time.\"),\n",
    "    (\"brb\", \"I will return shortly.\"),\n",
    "    (\"idk what to do\", \"I am uncertain about how to proceed.\"),\n",
    "    (\"thx for the help\", \"Thank you for your assistance.\"),\n",
    "    (\"np\", \"You are welcome.\"),\n",
    "    (\"gtg\", \"I must leave now.\"),\n",
    "    (\"omg that's bad\", \"That is quite concerning.\"),\n",
    "]\n",
    "\n",
    "extended_pairs.extend(variations)\n",
    "\n",
    "# Split into train/validation/test\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len(extended_pairs))\n",
    "train_size = int(0.7 * len(extended_pairs))\n",
    "val_size = int(0.15 * len(extended_pairs))\n",
    "\n",
    "train_pairs = [extended_pairs[i] for i in indices[:train_size]]\n",
    "val_pairs = [extended_pairs[i] for i in indices[train_size:train_size+val_size]]\n",
    "test_pairs = [extended_pairs[i] for i in indices[train_size+val_size:]]\n",
    "\n",
    "print(f\"Training samples: {len(train_pairs)}\")\n",
    "print(f\"Validation samples: {len(val_pairs)}\")\n",
    "print(f\"Test samples: {len(test_pairs)}\")\n",
    "\n",
    "# Example pairs\n",
    "print(\"\\nExample training pairs:\")\n",
    "for i in range(3):\n",
    "    print(f\"Informal: {train_pairs[i][0]}\")\n",
    "    print(f\"Formal: {train_pairs[i][1]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5188b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Hugging Face datasets\n",
    "def create_dataset(pairs):\n",
    "    return Dataset.from_dict({\n",
    "        \"informal\": [p[0] for p in pairs],\n",
    "        \"formal\": [p[1] for p in pairs]\n",
    "    })\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": create_dataset(train_pairs),\n",
    "    \"validation\": create_dataset(val_pairs),\n",
    "    \"test\": create_dataset(test_pairs)\n",
    "})\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bdc74d",
   "metadata": {},
   "source": [
    "## Step 2: Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8efcab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"google/flan-t5-base\"  # or \"t5-base\" or \"google/flan-t5-small\" for faster training\n",
    "OUTPUT_DIR = \"./models/informal-to-formal-t5\"\n",
    "MAX_INPUT_LENGTH = 128\n",
    "MAX_TARGET_LENGTH = 128\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e851f741",
   "metadata": {},
   "source": [
    "## Step 3: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7b79e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    # Add task prefix for T5\n",
    "    inputs = [f\"Convert informal to formal: {text}\" for text in examples[\"informal\"]]\n",
    "    targets = examples[\"formal\"]\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    labels = tokenizer(\n",
    "        targets,\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"informal\", \"formal\"]\n",
    ")\n",
    "\n",
    "print(\"Tokenized dataset:\")\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd018f23",
   "metadata": {},
   "source": [
    "## Step 4: Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad524ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "# Load metrics\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    predictions, labels = eval_preds\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in labels (used for padding)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Clean up text\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    \n",
    "    # Compute BLEU\n",
    "    bleu_result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    \n",
    "    # Compute ROUGE\n",
    "    decoded_labels_flat = [label[0] for label in decoded_labels]\n",
    "    rouge_result = rouge_metric.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels_flat\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"bleu\": bleu_result[\"score\"],\n",
    "        \"rouge1\": rouge_result[\"rouge1\"],\n",
    "        \"rouge2\": rouge_result[\"rouge2\"],\n",
    "        \"rougeL\": rouge_result[\"rougeL\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb771fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments - 100 epochs as requested\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=100,  # 100 epochs as requested\n",
    "    predict_with_generate=True,\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    greater_is_better=True,\n",
    "    warmup_steps=100,\n",
    "    gradient_accumulation_steps=2,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  FP16: {training_args.fp16}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58c570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a64977c",
   "metadata": {},
   "source": [
    "## Step 5: Train Model (100 Epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aba5964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "# Save the final model\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Final training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"Model saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c53b148",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1294599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "test_results = trainer.evaluate(tokenized_dataset[\"test\"])\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "for key, value in test_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# Save results\n",
    "with open(f\"{OUTPUT_DIR}/test_results.json\", \"w\") as f:\n",
    "    json.dump(test_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to {OUTPUT_DIR}/test_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbd12dd",
   "metadata": {},
   "source": [
    "## Step 7: Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fabdd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model for inference\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create text generation pipeline\n",
    "generator = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=OUTPUT_DIR,\n",
    "    tokenizer=OUTPUT_DIR,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Test examples\n",
    "test_examples = [\n",
    "    \"send me the file now\",\n",
    "    \"why didn't you finish the report?\",\n",
    "    \"need this asap\",\n",
    "    \"what's taking so long?\",\n",
    "    \"good job on this\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INFERENCE EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for informal_text in test_examples:\n",
    "    prompt = f\"Convert informal to formal: {informal_text}\"\n",
    "    result = generator(prompt, max_length=128, num_beams=4)\n",
    "    formal_text = result[0]['generated_text']\n",
    "    \n",
    "    print(f\"\\nInformal: {informal_text}\")\n",
    "    print(f\"Formal:   {formal_text}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13952385",
   "metadata": {},
   "source": [
    "## Step 8: Save Training Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414bf0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training metadata\n",
    "metadata = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"output_dir\": OUTPUT_DIR,\n",
    "    \"training_samples\": len(train_pairs),\n",
    "    \"validation_samples\": len(val_pairs),\n",
    "    \"test_samples\": len(test_pairs),\n",
    "    \"epochs\": training_args.num_train_epochs,\n",
    "    \"batch_size\": training_args.per_device_train_batch_size,\n",
    "    \"learning_rate\": training_args.learning_rate,\n",
    "    \"final_train_loss\": float(train_result.training_loss),\n",
    "    \"test_results\": test_results,\n",
    "    \"timestamp\": str(np.datetime64('now')),\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/training_metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"\\nTraining metadata saved!\")\n",
    "print(json.dumps(metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4228ef78",
   "metadata": {},
   "source": [
    "## Optional: View TensorBoard Logs\n",
    "\n",
    "To view training progress in TensorBoard:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir ./models/informal-to-formal-t5/logs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6171a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel saved to: {OUTPUT_DIR}\")\n",
    "print(f\"To use in production, update backend/model.py:\")\n",
    "print(f\"DEFAULT_MODEL = '{OUTPUT_DIR}'\")\n",
    "print(\"\\nTo view training logs:\")\n",
    "print(f\"tensorboard --logdir {OUTPUT_DIR}/logs\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
